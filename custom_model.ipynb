{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment setup\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "class ImageCompressionEnv(gym.Env):\n",
    "    def __init__(self, images):\n",
    "        super(ImageCompressionEnv, self).__init__()\n",
    "        self.images = images\n",
    "        self.current_index = 0\n",
    "        self.current_image = self.images[self.current_index]\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(101)  # Compression levels [0, 100]\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=self.current_image.shape, dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        compressed_image = self.compress_image(self.current_image, action)\n",
    "        reward = ssim(self.current_image, compressed_image)\n",
    "        self.current_index = (self.current_index + 1) % len(self.images)\n",
    "        self.current_image = self.images[self.current_index]\n",
    "        done = self.current_index == 0\n",
    "        return compressed_image, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        self.current_image = self.images[self.current_index]\n",
    "        return self.current_image\n",
    "\n",
    "    def compress_image(self, image : np.ndarray, compression_level):\n",
    "        print(image.shape)\n",
    "        image = Image.fromarray(image)\n",
    "        buffer = io.BytesIO()\n",
    "        image.save(buffer, format=\"JPEG\", quality=compression_level)\n",
    "        buffer.seek(0)\n",
    "        r = buffer.getvalue()\n",
    "        r = Image.open(io.BytesIO(r))\n",
    "        r = np.array(r)\n",
    "        print(r.shape)\n",
    "        return r\n",
    "\n",
    "# Example usage\n",
    "images = [np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8) for _ in range(100)]\n",
    "env = ImageCompressionEnv(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO algorithm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.policy_layer = nn.Linear(128, output_dim)\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        policy = self.policy_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "        return policy, value\n",
    "\n",
    "    def get_action(self, state):\n",
    "        policy, _ = self.forward(state)\n",
    "        policy_dist = Categorical(logits=policy)\n",
    "        action = policy_dist.sample()\n",
    "        return action.item(), policy_dist.log_prob(action), policy_dist.entropy()\n",
    "\n",
    "    def evaluate_action(self, state, action):\n",
    "        policy, value = self.forward(state)\n",
    "        policy_dist = Categorical(logits=policy)\n",
    "        action_log_probs = policy_dist.log_prob(action)\n",
    "        dist_entropy = policy_dist.entropy()\n",
    "        return action_log_probs, torch.squeeze(value), dist_entropy\n",
    "\n",
    "def compute_gae(rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "def ppo_update(agent, optimizer, trajectories, clip_param=0.2):\n",
    "    states = torch.stack([trajectory[0] for trajectory in trajectories])\n",
    "    actions = torch.tensor([trajectory[1] for trajectory in trajectories])\n",
    "    log_probs_old = torch.tensor([trajectory[2] for trajectory in trajectories])\n",
    "    returns = torch.tensor([trajectory[3] for trajectory in trajectories])\n",
    "    advantages = returns - torch.tensor([trajectory[4] for trajectory in trajectories])\n",
    "    \n",
    "    for _ in range(4):\n",
    "        log_probs, state_values, dist_entropy = agent.evaluate_action(states, actions)\n",
    "        ratio = torch.exp(log_probs - log_probs_old)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - state_values).pow(2).mean()\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * dist_entropy.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Example usage\n",
    "input_dim = 64 * 64 * 3  # Example image size\n",
    "output_dim = 101  # Compression levels from 0 to 100\n",
    "agent = PPO(input_dim, output_dim)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "(64, 64, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_rewards\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, agent, optimizer, num_episodes, gamma, clip_param)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     12\u001b[0m     action, log_prob, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m---> 13\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m, in \u001b[0;36mImageCompressionEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     21\u001b[0m     compressed_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompress_image(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_image, action)\n\u001b[1;32m---> 22\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressed_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index]\n",
      "File \u001b[1;32mc:\\Users\\a_misc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\metrics\\_structural_similarity.py:186\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[1;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         win_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((np\u001b[38;5;241m.\u001b[39masarray(im1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m win_size) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin_size exceeds image extent. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither ensure that your images are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least 7x7; or pass win_size explicitly \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min the function call, with an odd value \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless than or equal to the smaller side of your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages. If your images are multichannel \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(with color channels), set channel_axis to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe axis number corresponding to the channels.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (win_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindow size must be odd.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train(env, agent, optimizer, num_episodes=1000, gamma=0.99, clip_param=0.2):\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        trajectories = []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob, _ = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state.flatten()).unsqueeze(0)\n",
    "\n",
    "            mask = 1 if not done else 0\n",
    "            trajectories.append((state, action, log_prob, reward, mask))\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        states = torch.stack([trajectory[0] for trajectory in trajectories])\n",
    "        actions = torch.tensor([trajectory[1] for trajectory in trajectories])\n",
    "        log_probs = torch.tensor([trajectory[2] for trajectory in trajectories])\n",
    "        rewards = torch.tensor([trajectory[3] for trajectory in trajectories])\n",
    "        masks = torch.tensor([trajectory[4] for trajectory in trajectories])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, next_value = agent(states[-1])\n",
    "            values = torch.cat([agent(states)[1], next_value.unsqueeze(0)])\n",
    "\n",
    "        returns = compute_gae(rewards, masks, values, gamma)\n",
    "        ppo_update(agent, optimizer, trajectories, clip_param)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "# Example usage\n",
    "train(env, agent, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(env, agent, num_episodes=100):\n",
    "    all_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _, _ = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state.flatten()).unsqueeze(0)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        all_rewards.append(episode_reward)\n",
    "    return all_rewards\n",
    "\n",
    "# Example usage\n",
    "evaluation_rewards = evaluate(env, agent)\n",
    "print(f\"Average evaluation reward: {np.mean(evaluation_rewards)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
