{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "import logging\n",
    "import cv2\n",
    "\n",
    "MAX_INT = 2**31 - 1\n",
    "logger = logging.getLogger(\"ENOFT_LOGER\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(\"model.json\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "class ImageCompressionEnv(gym.Env):\n",
    "    def __init__(self, images, org_images):\n",
    "        super(ImageCompressionEnv, self).__init__()\n",
    "        self.images = images\n",
    "        self.current_index = 0\n",
    "        self.current_image = self.images[self.current_index]\n",
    "        self.org_images = org_images\n",
    "        self.original_size = 0\n",
    "        self.compression_ratio = 1.0  # Initialize with a default value\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.MultiDiscrete([101, 65536, 101, 101, 5])\n",
    "        feature_dim = len(self.current_image) + 1  # Include compression ratio in the state\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=np.inf, shape=(feature_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        compressed_image, compressed_size = self.compress_image(self.org_images[self.current_index], action)\n",
    "        max_size = self.original_size * self.compression_ratio / 100\n",
    "\n",
    "        grey_original = cv2.cvtColor(self.org_images[self.current_index], cv2.COLOR_BGR2GRAY)\n",
    "        grey_compressed = cv2.cvtColor(compressed_image, cv2.COLOR_BGR2GRAY)\n",
    "        got_ssim = ssim(grey_original, grey_compressed, multichannel=True)\n",
    "\n",
    "        # Weighted reward calculation\n",
    "        ssim_weight = 0.7\n",
    "        size_weight = 30\n",
    "        size_penalty = (compressed_size - max_size) / max_size if compressed_size > max_size else 0\n",
    "\n",
    "        reward = ssim_weight * got_ssim - size_weight * size_penalty\n",
    "\n",
    "        self.current_index = (self.current_index + 1) % len(self.images)\n",
    "        done = self.current_index == 0\n",
    "\n",
    "        state = self.update_environment_state()\n",
    "        log = {\n",
    "            \"original_size\": self.original_size,\n",
    "            \"compressed_size\": compressed_size,\n",
    "            \"max_size\": max_size,\n",
    "            \"got_ssim\": got_ssim,\n",
    "            \"reward\": reward,\n",
    "            \"action\": action\n",
    "        }\n",
    "        logger.info(log)\n",
    "        return state, float(reward), done, {}\n",
    "\n",
    "    def update_environment_state(self):\n",
    "        self.current_image = self.images[self.current_index]\n",
    "        org_img = self.org_images[self.current_index]\n",
    "        success, buffer = cv2.imencode('.jpg', org_img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "        buffer = io.BytesIO(buffer)\n",
    "        self.original_size = len(buffer.getvalue())\n",
    "        self.compression_ratio = int(np.random.uniform(30, 100))\n",
    "        state = np.append(self.current_image, self.compression_ratio)\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        return self.update_environment_state()\n",
    "\n",
    "    def compress_image(self, image: np.ndarray, action):\n",
    "        compression_level, rst_interval, luma_quality, chroma_quality, sampling_factor = action\n",
    "\n",
    "        sampling_map = {\n",
    "            0: cv2.IMWRITE_JPEG_SAMPLING_FACTOR_411,\n",
    "            1: cv2.IMWRITE_JPEG_SAMPLING_FACTOR_420,\n",
    "            2: cv2.IMWRITE_JPEG_SAMPLING_FACTOR_422,\n",
    "            3: cv2.IMWRITE_JPEG_SAMPLING_FACTOR_440,\n",
    "            4: cv2.IMWRITE_JPEG_SAMPLING_FACTOR_444\n",
    "        }\n",
    "        sampling_factor = np.clip(sampling_factor, 0, 4)\n",
    "        sampling = sampling_map[sampling_factor]\n",
    "\n",
    "        encode_params = [\n",
    "            int(cv2.IMWRITE_JPEG_QUALITY), compression_level,\n",
    "            int(cv2.IMWRITE_JPEG_RST_INTERVAL), rst_interval,\n",
    "            int(cv2.IMWRITE_JPEG_LUMA_QUALITY), luma_quality,\n",
    "            int(cv2.IMWRITE_JPEG_CHROMA_QUALITY), chroma_quality,\n",
    "            int(cv2.IMWRITE_JPEG_SAMPLING_FACTOR), sampling\n",
    "        ]\n",
    "        success, buffer = cv2.imencode('.jpg', image, encode_params)\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(\"Failed to compress image\")\n",
    "\n",
    "        buffer = io.BytesIO(buffer)\n",
    "        compressed_size = len(buffer.getvalue())\n",
    "\n",
    "        buffer.seek(0)\n",
    "        compressed_image = Image.open(buffer)\n",
    "        compressed_image = np.array(compressed_image)\n",
    "\n",
    "        return compressed_image, compressed_size\n",
    "\n",
    "\n",
    "png_folder = \"datasets/kaggle_Kodak/train\"\n",
    "images = []\n",
    "org_images = []\n",
    "for filename in os.listdir(png_folder):\n",
    "    img_path = os.path.join(png_folder, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    feature_vector = []\n",
    "\n",
    "    # Add height and width\n",
    "    h, w, c = img.shape\n",
    "    feature_vector.append(h)\n",
    "    feature_vector.append(w)\n",
    "\n",
    "    # Add aspect ratio\n",
    "    feature_vector.append(h / w)\n",
    "\n",
    "    # Add mean and std of each channel\n",
    "    for i in range(c):\n",
    "        mean = np.mean(img[:, :, i])\n",
    "        std = np.std(img[:, :, i])\n",
    "        feature_vector.append(mean)\n",
    "        feature_vector.append(std)\n",
    "\n",
    "    # Add entropy\n",
    "    hist, _ = np.histogram(img.flatten(), bins=256, range=[0, 256])\n",
    "    hist = hist / np.sum(hist)\n",
    "    entropy = -np.sum(hist * np.log2(hist + 1e-6))\n",
    "    feature_vector.append(entropy)\n",
    "\n",
    "    # Add edge density\n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    edge_density = np.mean(edges)\n",
    "    feature_vector.append(edge_density)\n",
    "\n",
    "    # Add brightness\n",
    "    brightness = np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "    feature_vector.append(brightness)\n",
    "\n",
    "    # Add contrast\n",
    "    contrast = np.std(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "    feature_vector.append(contrast)\n",
    "\n",
    "    # Add sharpness\n",
    "    sharpness = np.mean(cv2.Laplacian(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), cv2.CV_64F))\n",
    "    feature_vector.append(sharpness)\n",
    "\n",
    "    # Add saturation\n",
    "    saturation = np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:, :, 1])\n",
    "    feature_vector.append(saturation)\n",
    "\n",
    "    images.append(feature_vector)\n",
    "    org_images.append(img)\n",
    "\n",
    "env = ImageCompressionEnv(images, org_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Proximal Policy Optimization\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, input_dim, output_dims):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.policy_layers = nn.ModuleList([\n",
    "            nn.Linear(128, 101),    # For first action dimension (101 values)\n",
    "            nn.Linear(128, 65536),  # For second action dimension (65536 values)\n",
    "            nn.Linear(128, 101),    # For third action dimension (101 values)\n",
    "            nn.Linear(128, 101),    # For fourth action dimension (101 values)\n",
    "            nn.Linear(128, 5)       # For fifth action dimension (5 values)\n",
    "        ])\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "        self.output_dims = output_dims  # List of number of action values per dimension\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        policy_logits = [layer(x) for layer in self.policy_layers]\n",
    "        value = self.value_layer(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def get_action(self, state):\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        for logits in policy_logits:\n",
    "            policy_dist = Categorical(logits=logits)\n",
    "            action = policy_dist.sample()\n",
    "            actions.append(action.item())\n",
    "            log_probs.append(policy_dist.log_prob(action))\n",
    "            entropies.append(policy_dist.entropy())\n",
    "        return actions, torch.stack(log_probs), torch.stack(entropies)\n",
    "\n",
    "    def evaluate_action(self, state, action):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        for i, logits in enumerate(policy_logits):\n",
    "            policy_dist = Categorical(logits=logits)\n",
    "            log_prob = policy_dist.log_prob(action[:, i])\n",
    "            log_probs.append(log_prob)\n",
    "            entropies.append(policy_dist.entropy())\n",
    "        return torch.stack(log_probs, dim=1), torch.squeeze(value), torch.stack(entropies, dim=1)\n",
    "\n",
    "def compute_gae(rewards, masks, values, gamma, lam=0.95):\n",
    "    values = values.squeeze()\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return torch.tensor(returns)\n",
    "\n",
    "def ppo_update(agent, optimizer, trajectories, clip_param, gamma):\n",
    "    states = torch.cat([trajectory[0] for trajectory in trajectories]).detach()\n",
    "    actions = torch.stack([torch.tensor(trajectory[1]) for trajectory in trajectories]).detach()\n",
    "    log_probs = torch.stack([trajectory[2] for trajectory in trajectories]).detach()\n",
    "    rewards = torch.tensor([trajectory[3] for trajectory in trajectories], dtype=torch.float32)\n",
    "    masks = torch.tensor([trajectory[4] for trajectory in trajectories], dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, values = agent(states)\n",
    "        values = values.squeeze()\n",
    "    values = torch.cat((values, torch.zeros(1, dtype=values.dtype)))  # Add the value for the final state\n",
    "    returns = compute_gae(rewards, masks, values, gamma=gamma)\n",
    "    \n",
    "    advantages = returns - values[:-1]\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "    \n",
    "    for _ in range(4):  # Optimize policy for K epochs\n",
    "        policy_logits, _ = agent(states)\n",
    "        \n",
    "        new_log_probs = []\n",
    "        for i, logits in enumerate(policy_logits):\n",
    "            dist = Categorical(logits=logits)\n",
    "            new_log_prob = dist.log_prob(actions[:, i])\n",
    "            new_log_probs.append(new_log_prob)\n",
    "        new_log_probs = torch.stack(new_log_probs, dim=1).sum(dim=1)\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - log_probs.sum(dim=1))\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        value_loss = nn.MSELoss()(returns, values[:-1])\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * new_log_probs.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 7.536042101004448\n",
      "Episode 10, Reward: 12.20064886701238\n",
      "Episode 20, Reward: -6.4791647041845515\n",
      "Episode 30, Reward: 10.161764210289704\n",
      "Episode 40, Reward: 10.064753092589733\n",
      "Episode 50, Reward: 10.31331581751886\n",
      "Episode 60, Reward: 8.413726598551383\n",
      "Episode 70, Reward: 10.090268042313461\n",
      "Episode 80, Reward: 9.879557151480846\n",
      "Episode 90, Reward: 10.617147391162721\n",
      "Episode 100, Reward: 9.504449301906616\n",
      "Episode 110, Reward: -31.309742075105966\n",
      "Episode 120, Reward: -98.0719115759418\n",
      "Episode 130, Reward: -81.77643241181316\n",
      "Episode 140, Reward: -104.25828426128892\n",
      "Episode 150, Reward: -20.82367630145023\n",
      "Episode 160, Reward: -23.31499948305776\n",
      "Episode 170, Reward: -9.00556477990353\n",
      "Episode 180, Reward: -42.69874558275102\n",
      "Episode 190, Reward: -38.591498029570815\n",
      "Episode 200, Reward: -77.80231287273577\n",
      "Episode 210, Reward: -61.56921916997036\n",
      "Episode 220, Reward: 3.3864150082949775\n",
      "Episode 230, Reward: -31.638835019312133\n",
      "Episode 240, Reward: -47.376460164356295\n",
      "Episode 250, Reward: 3.3483025803624376\n",
      "Episode 260, Reward: -51.26364440368369\n",
      "Episode 270, Reward: -14.091456087003925\n",
      "Episode 280, Reward: -18.793497912810153\n",
      "Episode 290, Reward: -27.442330436210447\n",
      "Episode 300, Reward: -58.148265691788154\n",
      "Episode 310, Reward: -20.690135482923658\n",
      "Episode 320, Reward: -44.027130021456536\n",
      "Episode 330, Reward: -14.163794128715669\n",
      "Episode 340, Reward: -28.13094818191285\n",
      "Episode 350, Reward: -25.405591285915815\n",
      "Episode 360, Reward: -66.21118796092424\n",
      "Episode 370, Reward: -0.8904186317423106\n",
      "Episode 380, Reward: 2.6690431095361387\n",
      "Episode 390, Reward: 2.6487062901424894\n",
      "Episode 400, Reward: -4.389451235958054\n",
      "Episode 410, Reward: 0.2939134241454985\n",
      "Episode 420, Reward: -42.14701403110888\n",
      "Episode 430, Reward: -43.605829043305796\n",
      "Episode 440, Reward: -41.4792935862915\n",
      "Episode 450, Reward: -63.195169055556434\n",
      "Episode 460, Reward: -58.52140033847972\n",
      "Episode 470, Reward: -52.65214276263675\n",
      "Episode 480, Reward: -16.33624892592163\n",
      "Episode 490, Reward: -62.44906766327311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.536042101004448,\n",
       " 7.621902477993324,\n",
       " 9.064770315127621,\n",
       " 7.567444506816582,\n",
       " 10.890478730958653,\n",
       " 11.582953947478892,\n",
       " 10.79826918221752,\n",
       " 10.551752818026841,\n",
       " 12.018727300333406,\n",
       " 12.15389888090696,\n",
       " 12.20064886701238,\n",
       " 7.185988840938401,\n",
       " 7.8594551637369845,\n",
       " 12.045695826930801,\n",
       " -1.824278766791597,\n",
       " 2.5883785441828744,\n",
       " 7.844491641230423,\n",
       " 12.168036292075525,\n",
       " -18.948835977460234,\n",
       " 1.2480700057271061,\n",
       " -6.4791647041845515,\n",
       " -22.8475457621455,\n",
       " 10.713374912306296,\n",
       " 1.344336567141489,\n",
       " 10.895980402542776,\n",
       " 10.457935993131205,\n",
       " 9.023744224149057,\n",
       " 9.979481599278946,\n",
       " -3.3801551009560806,\n",
       " 2.632414933450627,\n",
       " 10.161764210289704,\n",
       " 9.507143618166939,\n",
       " 9.794860820176895,\n",
       " 10.977390290861278,\n",
       " 9.586443295563889,\n",
       " 6.8682802842090664,\n",
       " 10.1018064442513,\n",
       " 10.283384197610406,\n",
       " 9.669269448443519,\n",
       " 10.613396330811394,\n",
       " 10.064753092589733,\n",
       " 10.218713627366824,\n",
       " 8.971634452468754,\n",
       " 9.201877605931427,\n",
       " 9.973876636556831,\n",
       " 10.188565607333825,\n",
       " 10.477913294959501,\n",
       " 9.625081884367356,\n",
       " 10.071473358740134,\n",
       " 9.728443110767843,\n",
       " 10.31331581751886,\n",
       " 9.78178225278043,\n",
       " 10.247533381509434,\n",
       " 9.039755906166032,\n",
       " 9.873227853249702,\n",
       " 7.489796038881898,\n",
       " 9.562874037333586,\n",
       " 10.198529904067316,\n",
       " 10.289157929042416,\n",
       " 10.524692793318746,\n",
       " 8.413726598551383,\n",
       " 9.540550771312912,\n",
       " 10.050108563199133,\n",
       " -0.25852629101650104,\n",
       " 10.009932262697493,\n",
       " 9.528025695463743,\n",
       " -12.263096734240612,\n",
       " 9.608951486509,\n",
       " 9.684664080375116,\n",
       " 9.69924486531753,\n",
       " 10.090268042313461,\n",
       " 10.341205620153433,\n",
       " 9.814642058943663,\n",
       " 9.533728691191818,\n",
       " 9.42737124886364,\n",
       " 9.40609381831403,\n",
       " 10.629654947554403,\n",
       " 9.372656847174929,\n",
       " 10.087356715975877,\n",
       " 9.585420245473692,\n",
       " 9.879557151480846,\n",
       " 10.154349797108578,\n",
       " 10.436333040969213,\n",
       " 9.8810920832634,\n",
       " -54.737086614609765,\n",
       " 9.835915723596008,\n",
       " -12.608336186400052,\n",
       " 10.14333913752911,\n",
       " 9.505718307896979,\n",
       " 7.440623775010265,\n",
       " 10.617147391162721,\n",
       " 10.256790450207374,\n",
       " 10.440281229857579,\n",
       " 10.230353127990277,\n",
       " 10.7449899479612,\n",
       " 9.867492097518301,\n",
       " 10.036937902960146,\n",
       " 9.193851310422742,\n",
       " 9.89604148898525,\n",
       " 9.592557532500091,\n",
       " 9.504449301906616,\n",
       " 9.4288027266359,\n",
       " 9.177418924778417,\n",
       " 8.55084967128007,\n",
       " 7.347602342018594,\n",
       " 7.8115871715434215,\n",
       " 7.508457102564951,\n",
       " 7.646453825536724,\n",
       " 7.349187189588695,\n",
       " 7.565924946189063,\n",
       " -31.309742075105966,\n",
       " 7.836454228610257,\n",
       " -0.35747923721972386,\n",
       " -123.63152165114687,\n",
       " 2.055394506559053,\n",
       " -20.30669792995086,\n",
       " -48.20190638789864,\n",
       " -39.73876751215468,\n",
       " -42.858533571224584,\n",
       " -17.417513336925555,\n",
       " -98.0719115759418,\n",
       " -20.58286497363619,\n",
       " -22.15961354980042,\n",
       " -17.725954740129712,\n",
       " -27.528513718433075,\n",
       " -27.595740948206775,\n",
       " -19.6530910942261,\n",
       " -16.741658887929752,\n",
       " -5.5903673905213385,\n",
       " -42.76888476608744,\n",
       " -81.77643241181316,\n",
       " -25.574051524185663,\n",
       " 7.423815702139226,\n",
       " -19.86469630557276,\n",
       " 1.960135607377849,\n",
       " -18.334263754012007,\n",
       " -69.44817571234557,\n",
       " -35.55058276809614,\n",
       " 5.646955572027429,\n",
       " -36.74786560775499,\n",
       " -104.25828426128892,\n",
       " -37.76746042146185,\n",
       " -67.72243086536673,\n",
       " -54.315487676191275,\n",
       " -35.09924183888343,\n",
       " -51.88361287887623,\n",
       " -34.72671441924101,\n",
       " -85.94240299884764,\n",
       " -36.510082070933706,\n",
       " -82.98618980974824,\n",
       " -20.82367630145023,\n",
       " -40.764632802209555,\n",
       " -15.72538186661547,\n",
       " 8.174562885506184,\n",
       " -37.443456549295,\n",
       " -59.99167582332782,\n",
       " -30.274310395291103,\n",
       " -25.94666833278868,\n",
       " -9.70319993746452,\n",
       " -24.59937312087868,\n",
       " -23.31499948305776,\n",
       " -44.484309104455,\n",
       " -37.530596546150704,\n",
       " -23.712946159434455,\n",
       " -17.319263615573526,\n",
       " -43.56565976484071,\n",
       " -52.014976883219184,\n",
       " -34.78011535124737,\n",
       " -21.503935909255407,\n",
       " -18.45002234653454,\n",
       " -9.00556477990353,\n",
       " -55.127380914100975,\n",
       " -64.93308948666488,\n",
       " 11.247007297045258,\n",
       " -32.526821747758795,\n",
       " -77.83467405271122,\n",
       " -26.81601521744193,\n",
       " -68.08463483432955,\n",
       " 1.0699253161409503,\n",
       " 0.15971526224803323,\n",
       " -42.69874558275102,\n",
       " -73.17365035316973,\n",
       " -44.106043062005924,\n",
       " -36.656374743139835,\n",
       " -79.34201875197124,\n",
       " -30.093261716954682,\n",
       " -6.630018092579027,\n",
       " -49.2410375404598,\n",
       " -12.14924724991065,\n",
       " -58.75236176881818,\n",
       " -38.591498029570815,\n",
       " -32.014161839132065,\n",
       " -18.829500722428737,\n",
       " 9.581794251758165,\n",
       " -53.64883962132801,\n",
       " -56.94274527620727,\n",
       " -25.83758524357271,\n",
       " -22.999433222681088,\n",
       " -1.4224423779929685,\n",
       " -38.23917914571811,\n",
       " -77.80231287273577,\n",
       " -0.5679521702754795,\n",
       " -91.3243209926757,\n",
       " -75.19281459594406,\n",
       " 6.905039732660125,\n",
       " -41.738146980527375,\n",
       " -45.06256821089491,\n",
       " -26.63654798658661,\n",
       " 6.099782306137481,\n",
       " -51.02405299351341,\n",
       " -61.56921916997036,\n",
       " -41.488651933498936,\n",
       " -60.87497969448562,\n",
       " -20.199892818720674,\n",
       " -48.16520176344347,\n",
       " -13.40256245172621,\n",
       " -28.98212673776755,\n",
       " -13.538219400887206,\n",
       " -40.18864937587653,\n",
       " -61.09071530295005,\n",
       " 3.3864150082949775,\n",
       " -50.415226645463854,\n",
       " -10.574214069374404,\n",
       " -41.73973792486527,\n",
       " -24.66976356349377,\n",
       " -57.24611617178907,\n",
       " -77.89128899551979,\n",
       " -44.20719889047664,\n",
       " 7.810209474251932,\n",
       " -50.070585203072255,\n",
       " -31.638835019312133,\n",
       " -13.879656467887889,\n",
       " -95.42895285729361,\n",
       " -36.24841063450271,\n",
       " -43.26364988030394,\n",
       " -50.35988504571683,\n",
       " -26.651275172201338,\n",
       " -20.303739833273458,\n",
       " -31.720631795715256,\n",
       " -30.942310166621116,\n",
       " -47.376460164356295,\n",
       " 9.23865825379397,\n",
       " -10.763525788449844,\n",
       " -12.995671112690271,\n",
       " -46.25925743359907,\n",
       " -45.16183232067477,\n",
       " -85.88243359642226,\n",
       " -40.100033888088426,\n",
       " -30.00049983199976,\n",
       " 6.33904309540534,\n",
       " 3.3483025803624376,\n",
       " -83.44165409238894,\n",
       " -55.3205132462676,\n",
       " -14.481261661518788,\n",
       " -19.566543091271903,\n",
       " -108.18515077536738,\n",
       " -51.71788447706799,\n",
       " -14.982193649165778,\n",
       " -31.257956166516106,\n",
       " -73.81483182592096,\n",
       " -51.26364440368369,\n",
       " 9.261914382817384,\n",
       " -0.4970685014954531,\n",
       " -31.4640027183246,\n",
       " -30.509427343767467,\n",
       " -0.344744451963146,\n",
       " -8.45112466386927,\n",
       " 11.940527058215975,\n",
       " -20.726433969839064,\n",
       " -7.345934536356968,\n",
       " -14.091456087003925,\n",
       " -18.93006653593487,\n",
       " -1.8304112304194384,\n",
       " -22.157493130153114,\n",
       " 11.652445287484598,\n",
       " -7.289829941784867,\n",
       " -5.757333794725407,\n",
       " -25.210692878725727,\n",
       " -72.76476168760334,\n",
       " -16.558124888620025,\n",
       " -18.793497912810153,\n",
       " -105.7710224283102,\n",
       " -20.12204198112189,\n",
       " -87.39473647984687,\n",
       " -81.48473582313933,\n",
       " 11.161427721652574,\n",
       " -10.343011148999773,\n",
       " -12.40099836075886,\n",
       " -48.18106226231365,\n",
       " -1.1790235287183704,\n",
       " -27.442330436210447,\n",
       " -45.408384680201145,\n",
       " -10.109612583874911,\n",
       " 9.275004994532667,\n",
       " -30.915890694364627,\n",
       " -27.748085476968583,\n",
       " -47.260733666129056,\n",
       " -13.188484726982601,\n",
       " -17.80966722646212,\n",
       " 7.194325391235902,\n",
       " -58.148265691788154,\n",
       " -27.24385630938148,\n",
       " -21.479238530098065,\n",
       " -9.581366157802973,\n",
       " -74.45043539987039,\n",
       " -29.58015148227495,\n",
       " -9.476055938951223,\n",
       " 5.097635454619615,\n",
       " -16.38474686520312,\n",
       " -19.192515835362684,\n",
       " -20.690135482923658,\n",
       " -56.61605241400144,\n",
       " -32.75612586063309,\n",
       " -95.409298615394,\n",
       " -20.311118740269404,\n",
       " -6.930522790036767,\n",
       " -61.6528521995735,\n",
       " -24.13566683220967,\n",
       " -69.82847444496446,\n",
       " -45.006719356159046,\n",
       " -44.027130021456536,\n",
       " -13.751484452976301,\n",
       " -21.50689580233317,\n",
       " -33.542799258263905,\n",
       " -32.31745086260798,\n",
       " -47.642714040428054,\n",
       " -75.76783743945376,\n",
       " -6.696201049246939,\n",
       " -19.176531857607625,\n",
       " -29.647919059798742,\n",
       " -14.163794128715669,\n",
       " -10.592308674969523,\n",
       " -13.80702496206331,\n",
       " -90.98030829898669,\n",
       " -30.52140493532055,\n",
       " -2.8963208750963227,\n",
       " -60.24214123608199,\n",
       " -16.864336643102607,\n",
       " -19.691178468988955,\n",
       " -58.10624874064783,\n",
       " -28.13094818191285,\n",
       " -32.74257036466747,\n",
       " -46.608241672330195,\n",
       " -26.344605984270277,\n",
       " -51.49692313302317,\n",
       " -84.04221819191119,\n",
       " -13.726443752334049,\n",
       " 6.27942867631326,\n",
       " -130.71485541109118,\n",
       " -34.65178352070868,\n",
       " -25.405591285915815,\n",
       " -70.06544309126512,\n",
       " 3.7610444823708535,\n",
       " -28.407028509274728,\n",
       " 3.7274602018422094,\n",
       " -1.7479799449553752,\n",
       " 11.208027204138155,\n",
       " -42.44029112562048,\n",
       " -42.56997138745613,\n",
       " -20.869565815646762,\n",
       " -66.21118796092424,\n",
       " -0.7432165514336581,\n",
       " -48.14677928417538,\n",
       " -15.157669104740632,\n",
       " 8.820032706108003,\n",
       " -54.08564229173468,\n",
       " -12.464989467127639,\n",
       " -1.0887207537287287,\n",
       " -53.60637740228266,\n",
       " -53.797295344751625,\n",
       " -0.8904186317423106,\n",
       " -18.772989723392854,\n",
       " -30.354433305844665,\n",
       " -36.89882810394811,\n",
       " -19.26870827434797,\n",
       " -2.741334860602661,\n",
       " -38.53289572555762,\n",
       " -58.75888497447422,\n",
       " 1.180784892309444,\n",
       " -4.339417596960973,\n",
       " 2.6690431095361387,\n",
       " -33.93406256390111,\n",
       " -11.61032768957954,\n",
       " -20.73185298923186,\n",
       " -13.711161202844577,\n",
       " -90.00670191467631,\n",
       " -19.056794674690515,\n",
       " -33.15688127132134,\n",
       " -43.183796050703634,\n",
       " -17.90392706339558,\n",
       " 2.6487062901424894,\n",
       " -53.04997058070672,\n",
       " -67.94669285437087,\n",
       " -28.452951538200875,\n",
       " -24.480559896872784,\n",
       " -6.100236121363525,\n",
       " -59.130389713004014,\n",
       " -25.038479731100473,\n",
       " -26.43150006453395,\n",
       " -18.836780575085406,\n",
       " -4.389451235958054,\n",
       " -35.30423873486887,\n",
       " -18.897473619351715,\n",
       " -80.79230327586727,\n",
       " -32.43139455961984,\n",
       " -5.5087412237927635,\n",
       " -45.07086144251131,\n",
       " -30.29877915769832,\n",
       " -38.13384773792536,\n",
       " -13.680171445909405,\n",
       " 0.2939134241454985,\n",
       " 10.63806803222811,\n",
       " -40.03685158419117,\n",
       " -18.034021206163608,\n",
       " -32.90862251925509,\n",
       " -12.91978627446589,\n",
       " -8.374677640255307,\n",
       " -24.307061290024077,\n",
       " -39.12283167764059,\n",
       " -17.57512205975825,\n",
       " -42.14701403110888,\n",
       " -11.146031328829741,\n",
       " -74.26713931873768,\n",
       " -56.78682701695728,\n",
       " -13.887186442576903,\n",
       " -33.21792540689508,\n",
       " -18.73478687169081,\n",
       " -34.35647473131229,\n",
       " -65.88195976111066,\n",
       " -13.32917043475384,\n",
       " -43.605829043305796,\n",
       " -123.76475850497872,\n",
       " -6.320066762273517,\n",
       " -60.84342571438878,\n",
       " -33.27782646187734,\n",
       " -52.51566536066518,\n",
       " -56.59283657762983,\n",
       " -67.58072859836098,\n",
       " -24.210260727096532,\n",
       " -13.742942811395958,\n",
       " -41.4792935862915,\n",
       " -19.801540001528775,\n",
       " -1.8731889932607335,\n",
       " -55.18084004535223,\n",
       " -36.43930393124694,\n",
       " -25.072809148425197,\n",
       " -29.464610121774903,\n",
       " 11.032879578566133,\n",
       " 6.353347648401542,\n",
       " -32.792000109815255,\n",
       " -63.195169055556434,\n",
       " -1.4522158914190484,\n",
       " -9.095967542040567,\n",
       " -63.995022475598084,\n",
       " -19.22249459805092,\n",
       " -1.987959432846969,\n",
       " -32.339703688011674,\n",
       " -3.1075786173808613,\n",
       " -11.409670225405012,\n",
       " -8.368073718427704,\n",
       " -58.52140033847972,\n",
       " -82.68942620271464,\n",
       " -49.925844560577524,\n",
       " -25.682214606917526,\n",
       " -12.317013636374414,\n",
       " 0.9980256970315665,\n",
       " -27.960388396459805,\n",
       " -46.31362511114519,\n",
       " -7.638792034235138,\n",
       " -33.13284513886624,\n",
       " -52.65214276263675,\n",
       " -33.264127616947945,\n",
       " -15.158200179476616,\n",
       " -57.81540188732615,\n",
       " -50.54422824308737,\n",
       " -42.20422974188634,\n",
       " -21.699037617922645,\n",
       " 11.783564787080836,\n",
       " -38.28550618974225,\n",
       " -6.367477464536078,\n",
       " -16.33624892592163,\n",
       " -32.449915410592,\n",
       " -33.36148815317728,\n",
       " -10.248284542340363,\n",
       " -65.53292518425165,\n",
       " -20.837472084363476,\n",
       " -42.82349989856367,\n",
       " -5.36710261134004,\n",
       " -15.871040523391123,\n",
       " -97.67844826806468,\n",
       " -62.44906766327311,\n",
       " -26.400969306032092,\n",
       " -14.936398475287573,\n",
       " -16.002192522842986,\n",
       " -63.19294927327813,\n",
       " -21.52290183874888,\n",
       " -53.36219338003179,\n",
       " -5.361199117040224,\n",
       " -91.76477652032065,\n",
       " -17.43269765212019]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(env, agent, optimizer, num_episodes=500, gamma=0.99, clip_param=0.2):\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        trajectories = []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob, _ = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "            mask = 1 if not done else 0\n",
    "            trajectories.append((state, action, log_prob, reward, mask))\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        ppo_update(agent, optimizer, trajectories, clip_param, gamma)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "            torch.save(agent.state_dict(), f\"model_episode_{episode}_reward_{episode_reward}.pth\")\n",
    "            \n",
    "    return all_rewards\n",
    "\n",
    "input_dim = 15 + 1  # Example feature size plus the compression ratio\n",
    "output_dims = [101, 65536, 101, 101, 5]  # Matching the action space dimensions\n",
    "agent = PPO(input_dim, len(output_dims))\n",
    "optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "train(env, agent, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, num_episodes=10):\n",
    "    all_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _, _ = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        all_rewards.append( episode_reward )\n",
    "    return all_rewards\n",
    "\n",
    "# Example usage\n",
    "# evaluation_rewards = evaluate(env, agent)\n",
    "# print(f\"Average evaluation reward: {np.mean(evaluation_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(agent, path):\n",
    "    torch.save(agent.state_dict(), path)\n",
    "    \n",
    "def load_model(agent, path):\n",
    "    agent.load_state_dict(torch.load(path))\n",
    "    return agent\n",
    "\n",
    "# Example usage\n",
    "save_model(agent, \"model.pth\")\n",
    "# loaded_agent = PPO(input_dim, output_dim)\n",
    "# loaded_agent = load_model(loaded_agent, \"model.pth\")\n",
    "# evaluation_rewards = evaluate(env, loaded_agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
